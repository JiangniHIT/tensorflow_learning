#首先声明一个Checkpoint

checkpoint=tf.train.Checkpoint(model=model)

#tf.train.Checkpoint()接受的初始化参数比较特殊，是一个**kwargs，具体而言，是一系列的键值对，键名可以随意取，值为需要保存的对象。
#例如，保存一个继承tf.keras.Model的模型实例model和一个继承tf.train.Optimizer的优化器optimizer，

checkpoint=tf.train.Checkpoint(myAwesomeModel=model,myAwesomeOptimizer=optimizer)

#这里的myAwesomeModel是我们为待保存的模型model所取的任意键名。
#注意，在恢复变量的时候，还要使用这一键名
#接下来，当模型训练完需要保存的时候，使用：

checkpoint.save(save_path_with_prefix)

#例如，在源代码目录建立一个名为save的文件夹并调用一次checkpoint.save('./save/model.ckpt')
#我们就可以在可以在 save 目录下发现名为checkpoint 、model.ckpt-1.index 、model.ckpt-1.data-00000-of-00001 的三个文件,
#这些文件就记录了变量信息。
#checkpoint.save() 方法可以运行多次,每运行一次都会得到一个.index 文件和.data 文件,序号依次累加。
#当在其他地方需要为模型重新载入之前保存的参数时,需要再次实例化一个 checkpoint,同时保持键名的一致。再调用 checkpoint 的 restore 方法。

model_to_be_restored=MyModel()    #待3恢复参数的同一模型
checkpoint=tf.train.Checkpoint(myAwesomeModel=model_to_be_restored)
checkpoint.restore(save_path_with_prefix_and_index)

#save_path_with_prefix_and_index 是之前保存的文件的目录 + 前缀 + 编号。
#调用 checkpoint.restore('./save/model.ckpt-1') 就可以载入前缀为 model.ckpt ,序号为 1 的文件来恢复模型。



###
#总体而言，恢复与保存变量的典型代码框架如下：
###

#在模型训练阶段：

model=MyModel()
checkpoint=tf.train.Checkpoint(myModel=model)#实例化checkpoint，指定保存对象为model（如果需要保存Optimizer的参数也可以加入）
#模型训练代码
checkpoint.save('./save/model.ckpt')#可以在模型训练结束后将参数保存到文件，也可以在模型训练过程中每隔一段时间就保存一次

#在模型使用阶段：

model=MyModel()
checkpoint=tf.train.Checkpoint(myModel=model)#实例化checkpoint，指定保存对象为model
checkpoint.restore(tf.train.latest_checkpoint('./save'))
#模型使用代码

##以MLP文件中的代码为例，展示模型变量的保存和载入：

import tensorflow as tf
import numpy as np
from zh.model.mlp.mlp import MLP
from zh.model.mlp.utils import DataLoader

tf.enable_eager_execution()
mode='test'
num_batches=1000
batch_size=50
learning_rate=0.001
data_loader=DataLoader()


def train():
    model=MLP()
    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)
    checkpoint=tf.train.Checkpoint(myAwesomeModel=model)
    for batch_index in range(num_batches):
        X,y=data_loader.get_batch(batch_size)
        with tf.GradientTape() as tape:
            y_logit_pred=model(tf.convert_to_tensor(X))
            loss=tf.losses.sparse_softmax_cross_entropy(labels=y,logits=y_logit_pred)
            print("batch %d: loss %f"%(batch_index,loss.numpy()))
        grads=tape.gradient(loss,model.variables)
        optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))
        if(batch_index+1)%100==0:                    #每隔100个batch保存一次
            checkpoint.save('./save/model.ckpt')
            
def test():
    model_to_be_restored=MLP()
    checkpoint=tf.train.Checkpoint(myAwesomeModel=model_to_be_restored)
    checkpoint.restore(tf.train.latest_checkpoint('./save'))
    num_eval_samples=np.shape(data_loader.eval_labels)[0]
    y_pred=model_to_be_restored.predict(tf.constant(data_loader.eval_data)).numpy()
    print("test accuracy: %f" % (sum(y_pred==data_loader.eval_labels)/num_eval_samples))
    
if __name__=='__main__':
    if mode=='train':
        train()
    if mode=='test':
        test()
