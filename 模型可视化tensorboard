import tensorflow as tf
import numpy as np

tf.enable_eager_execution()
#实现一个简单的DataLoader类来读取 MNIST 数据集数据
class DataLoader():
    def __init__(self):
        mnist=tf.contrib.learn.datasets.load_dataset("mnist")
        self.train_data=mnist.train.images
        self.train_labels=np.asarray(mnist.train.labels,dtype=np.int32)#将标签类型转换为int型
        self.eval_data=mnist.test.images
        self.eval_labels=np.asarray(mnist.test.labels,dtype=np.int32)#将标签类型转换为int型
        
    def get_batch(self,batch_size):
        index=np.random.randint(0,np.shape(self.train_data)[0],batch_size)
        return self.train_data[index,:],self.train_labels[index]
    
class MLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.dense1=tf.keras.layers.Dense(units=100,activation=tf.nn.relu)
        self.dense2=tf.keras.layers.Dense(units=10)
        
    def call(self,inputs):
        x=self.dense1(inputs)
        x=self.dense2(x)
        return x
    
    def predict(self,inputs):
        logits=self(inputs)
        return tf.argmax(logits,axis=-1)
    
#定义一些模型超参数

num_batches=1000
batch_size=50
learning_rate=0.001

#实例化模型、数据读取类和优化器：
model=MLP()
data_loader=DataLoader()
optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)
summary_writer=tf.contrib.summary.create_file_writer('./tensorboard')    #实例化记录器
with summary_writer.as_default(),tf.contrib.summary.always_record_summaries():
    #迭代进行一下步骤：
   #从DataLoader中随机取一批训练数据
#将这批数据送入模型，计算出模型的预测值
#将模型预测值与真实值进行比较，计算损失函数（loss）
#计算损失函数关于模型变量的导数
#使用优化器更新模型参数，以最小化损失函数
    for batch_index in range(num_batches):
        x,y=data_loader.get_batch(batch_size)
        with tf.GradientTape() as tape:
            y_logit_pred=model(tf.convert_to_tensor(x))
            loss=tf.losses.sparse_softmax_cross_entropy(labels=y,logits=y_logit_pred)
            print("batch %d: loss %f"%(batch_index,loss.numpy()))
            tf.contrib.summary.scalar("loss",loss,step=batch_index)   #记录当前loss
        grads=tape.gradient(loss,model.variables)
        optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))

